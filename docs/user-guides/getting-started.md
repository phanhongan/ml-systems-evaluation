# ğŸš€ Getting Started with ML Systems Evaluation Framework

This guide will help you quickly set up and run your first evaluation using the ML Systems Evaluation Framework.

## ğŸ”§ Prerequisites

- ğŸ Python 3.11 or higher
- ğŸ“¦ UV package manager (https://astral.sh/uv/)
- ğŸ“Š Access to your ML system's monitoring data
- ğŸ—ï¸ Basic understanding of your system's architecture

## ğŸ“¦ Installation

### 1ï¸âƒ£ Install the Framework

```bash
# Clone the repository
git clone <repository-url>
cd ml-systems-evaluation

# Install dependencies and the framework
uv sync --group dev

# (Optional) Activate the UV-managed virtual environment
uv shell

# For production installs (main dependencies only)
uv sync --group main
```

### 2ï¸âƒ£ Verify Installation

```bash
ml-eval --help
```

## âš¡ Quick Start: Your First Evaluation

### 1ï¸âƒ£ Use an Example Configuration

The framework provides industry-specific example configurations. For your first evaluation, we recommend starting with an existing example:

```bash
# Copy an example configuration for manufacturing
cp examples/industries/manufacturing/predictive-maintenance.yaml my-config.yaml

# Or create a new configuration from scratch
ml-eval create-config --output my-config.yaml --system-name "My Production System" --industry manufacturing
```

### 2ï¸âƒ£ Configure Your System

Create a configuration file for your system:

```yaml
# config.yaml
system:
  name: "Production Line Quality Control"
  criticality: "business-critical"

data_sources:
  - name: "quality_metrics"
    type: "database"
    connection: "postgresql://user:pass@localhost/quality_db"
    tables: ["quality_measurements", "defect_reports"]

collectors:
  - name: "quality_collector"
    type: "offline"
    data_source: "quality_metrics"
    metrics: ["accuracy", "precision", "recall", "f1_score"]

evaluators:
  - name: "performance_evaluator"
    type: "performance"
    thresholds:
      accuracy: 0.95
      precision: 0.90
      recall: 0.85

  - name: "drift_evaluator"
    type: "drift"
    detection_method: "statistical"
    sensitivity: 0.05

reports:
  - name: "business_report"
    type: "business"
    format: "html"
    output_path: "./reports/"

slo:
  availability: 0.999
  accuracy: 0.95
  latency_p95: 100  # milliseconds
```

### 3ï¸âƒ£ Run Your First Evaluation

```bash
# Validate your configuration first
ml-eval validate config.yaml

# Run a complete evaluation
ml-eval run config.yaml --output results.json

# Run specific components
ml-eval collect config.yaml --output data.json
ml-eval evaluate config.yaml --data data.json --output evaluation.json
ml-eval report config.yaml --results evaluation.json --output reports.json
```

### 4ï¸âƒ£ Review Results

Check the generated reports in the `./reports/` directory:

- **ğŸ“Š Business Report**: High-level metrics and recommendations
- **ğŸ“ˆ Performance Report**: Detailed performance analysis
- **ğŸ“‹ Compliance Report**: Regulatory compliance status

## ğŸ“Š Understanding Your Results

### ğŸ¯ Key Metrics to Monitor

1. **ğŸ“Š Accuracy**: Overall prediction accuracy
2. **ğŸ¯ Precision**: True positive rate
3. **ğŸ” Recall**: Sensitivity of the model
4. **ğŸ“ˆ Drift Score**: Data distribution changes
5. **âš¡ Latency**: Response time percentiles

### ğŸš¨ Alert Thresholds

The framework automatically alerts you when:
- ğŸ“‰ Performance metrics fall below thresholds
- ğŸ“Š Data drift is detected
- ğŸ”´ System availability drops
- ğŸ“‹ Compliance violations occur

## ğŸ¯ Next Steps

1. **âš™ï¸ Customize Configuration**: Adapt the template to your specific needs
2. **ğŸ“Š Set Up Monitoring**: Configure continuous monitoring
3. **ğŸ“‹ Define SLOs**: Establish Service Level Objectives
4. **ğŸ“ˆ Create Dashboards**: Visualize your metrics
5. **ğŸš¨ Set Up Alerts**: Configure notification systems

## ğŸ”§ Troubleshooting

### âŒ Common Issues

**ğŸš¨ Issue**: "No data found"
- **âœ… Solution**: Verify your data source configuration and connection

**ğŸš¨ Issue**: "Evaluation failed"
- **âœ… Solution**: Check your evaluator configuration and thresholds

**ğŸš¨ Issue**: "Template not found"
- **âœ… Solution**: Update to the latest version: `uv update`

### ğŸ†˜ Getting Help

- âš™ï¸ Check the [Configuration Guide](configuration.md) for detailed options
- ğŸ–¥ï¸ Review [CLI Reference](cli-reference.md) for command details
- ğŸ“‹ Consult [Example Configurations Guide](example-configurations.md) for your specific domain

## ğŸ’¡ Example: Manufacturing Quality Control

Here's a complete example for a manufacturing quality control system:

```yaml
# manufacturing-quality.yaml
system:
  name: "PCB Quality Control"
  criticality: "business-critical"

data_sources:
  - name: "quality_data"
    type: "database"
    connection: "postgresql://user:pass@localhost/pcb_quality"
    tables: ["inspection_results", "defect_logs"]

collectors:
  - name: "quality_metrics"
    type: "offline"
    data_source: "quality_data"
    metrics: ["accuracy", "false_positive_rate", "false_negative_rate"]

evaluators:
  - name: "quality_performance"
    type: "performance"
    thresholds:
      accuracy: 0.98
      false_positive_rate: 0.01
      false_negative_rate: 0.005

  - name: "quality_drift"
    type: "drift"
    detection_method: "ks_test"
    features: ["component_size", "solder_quality", "placement_accuracy"]

reports:
  - name: "quality_report"
    type: "business"
    format: "html"
    output_path: "./quality_reports/"

slo:
  availability: 0.9995
  accuracy: 0.98
  false_positive_rate: 0.01
  false_negative_rate: 0.005
```

Run this evaluation with:

```bash
ml-eval run manufacturing-quality.yaml --output quality-results.json
```

This will generate reports for your PCB quality control system, helping you maintain high quality standards and meet production targets. 