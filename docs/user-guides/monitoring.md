# ğŸ“Š Monitoring Setup

This guide helps you set up continuous monitoring for your ML systems using the ML Systems Evaluation Framework.

## ğŸ“‹ Overview

Continuous monitoring is essential for Industrial AI systems to:
- ğŸ” Detect performance degradation early
- ğŸ“ˆ Identify data drift before it impacts predictions
- ğŸ“‹ Ensure compliance with safety and business requirements
- ğŸš¨ Provide real-time alerts for critical issues

## âš™ï¸ Setting Up Continuous Monitoring

### ğŸ“Š Step 1: Configure Data Collection

Set up automated data collection from your ML system:

```yaml
# monitoring-config.yaml
data_sources:
  - name: "production_metrics"
    type: "database"
    connection: "postgresql://user:pass@localhost/prod_db"
    tables: ["predictions", "actuals", "system_metrics"]
    
  - name: "real_time_stream"
    type: "kafka"
    brokers: ["localhost:9092"]
    topics: ["ml-predictions", "ml-actuals"]

collectors:
  - name: "online_collector"
    type: "online"
    data_source: "real_time_stream"
    interval: "1m"
    metrics: ["accuracy", "latency", "throughput"]
    
  - name: "batch_collector"
    type: "offline"
    data_source: "production_metrics"
    schedule: "0 */6 * * *"  # Every 6 hours
    metrics: ["drift_score", "performance_trends"]
```

### ğŸ“‹ Step 2: Define Monitoring Rules

Configure what to monitor and when to alert:

```yaml
monitoring:
  rules:
    - name: "performance_degradation"
      condition: "accuracy < 0.95"
      window: "1h"
      severity: "critical"
      
    - name: "data_drift"
      condition: "drift_score > 0.1"
      window: "24h"
      severity: "warning"
      
    - name: "high_latency"
      condition: "latency_p95 > 100"
      window: "5m"
      severity: "critical"
      
    - name: "system_unavailable"
      condition: "availability < 0.999"
      window: "1m"
      severity: "critical"
```

### ğŸš¨ Step 3: Set Up Alerting

Configure notification channels:

```yaml
alerts:
  channels:
    - name: "slack"
      type: "slack"
      webhook: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
      
    - name: "email"
      type: "email"
      recipients: ["ml-team@company.com", "oncall@company.com"]
      
    - name: "pagerduty"
      type: "pagerduty"
      service_key: "YOUR_PAGERDUTY_KEY"

  routing:
    critical: ["slack", "pagerduty"]
    warning: ["slack"]
    info: ["email"]
```

### âš¡ Step 4: Start Monitoring

Run the monitoring service:

```bash
# Start continuous monitoring
ml-eval monitor --config monitoring-config.yaml

# Run in background
nohup ml-eval monitor --config monitoring-config.yaml > monitoring.log 2>&1 &

# Check status
ml-eval monitor --status
```

## ğŸ“ˆ Monitoring Dashboards

### ğŸ¨ Setting Up Dashboards

Create visualizations for your metrics:

```yaml
dashboards:
  - name: "ml_performance"
    type: "grafana"
    url: "http://localhost:3000"
    
  - name: "business_metrics"
    type: "custom"
    template: "business_dashboard.html"
```

### ğŸ¯ Key Dashboards

1. **ğŸ“Š Performance Dashboard**
   - Real-time accuracy, precision, recall
   - Latency percentiles
   - Throughput metrics

2. **ğŸ“ˆ Drift Dashboard**
   - Feature distribution changes
   - Statistical drift scores
   - Historical trends

3. **ğŸ’° Business Dashboard**
   - Revenue impact
   - Cost savings
   - Risk metrics

4. **ğŸ“‹ Compliance Dashboard**
   - Regulatory compliance status
   - Safety metrics
   - Audit trails

## ğŸš¨ Alert Management

### ğŸ”„ Alert Lifecycle

1. **ğŸ” Detection**: System detects violation of monitoring rules
2. **ğŸ“¢ Notification**: Alerts sent to configured channels
3. **âœ… Acknowledgment**: Team acknowledges the alert
4. **ğŸ” Investigation**: Root cause analysis
5. **ğŸ”§ Resolution**: Fix applied and verified
6. **ğŸ“ Post-mortem**: Document lessons learned

### ğŸ“ˆ Escalation Policies

```yaml
escalation:
  levels:
    - level: 1
      delay: "5m"
      channels: ["slack"]
      
    - level: 2
      delay: "15m"
      channels: ["slack", "email"]
      
    - level: 3
      delay: "30m"
      channels: ["slack", "email", "pagerduty"]
```

## ğŸ”Œ Integration with Existing Systems

### ğŸ“Š Prometheus Integration

```yaml
exporters:
  - name: "prometheus"
    type: "prometheus"
    port: 9090
    metrics:
      - "ml_accuracy"
      - "ml_latency"
      - "ml_drift_score"
```

### ğŸ“ˆ Grafana Integration

```yaml
dashboards:
  - name: "ml_overview"
    type: "grafana"
    datasource: "prometheus"
    panels:
      - title: "ML Performance"
        query: "ml_accuracy"
        type: "graph"
```

## ğŸ† Best Practices

### ğŸ“‹ Monitoring Strategy

1. **ğŸš€ Start Simple**: Begin with basic performance metrics
2. **ğŸ“ˆ Add Gradually**: Introduce drift detection and business metrics
3. **ğŸ§ª Test Alerts**: Verify alerting works before going live
4. **ğŸ“ Document Everything**: Keep runbooks for common issues
5. **ğŸ”„ Review Regularly**: Update thresholds based on system evolution

### âš¡ Performance Considerations

1. **ğŸ“Š Sampling**: Use sampling for high-volume data
2. **ğŸ“ˆ Aggregation**: Pre-aggregate metrics where possible
3. **ğŸ’¾ Retention**: Set appropriate data retention policies
4. **ğŸ“ˆ Scaling**: Plan for monitoring system scaling

### ğŸ›¡ï¸ Security

1. **ğŸ” Access Control**: Limit access to monitoring data
2. **ğŸ”’ Encryption**: Encrypt sensitive metrics in transit
3. **ğŸ“‹ Audit Logs**: Log all monitoring system access
4. **ğŸ“‹ Compliance**: Ensure monitoring meets regulatory requirements

## ğŸ”§ Troubleshooting

### âŒ Common Issues

**ğŸš¨ Issue**: "No data being collected"
- **âœ… Solution**: Check data source connections and permissions

**ğŸš¨ Issue**: "Alerts not firing"
- **âœ… Solution**: Verify monitoring rules and thresholds

**ğŸš¨ Issue**: "High resource usage"
- **âœ… Solution**: Optimize collection intervals and sampling

### ğŸ†˜ Getting Help

- âš™ï¸ Check the [Configuration Guide](configuration.md) for detailed options
- ğŸ–¥ï¸ Review [CLI Reference](cli-reference.md) for command details 