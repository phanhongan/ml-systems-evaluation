# Testing Guide for ML Systems Evaluation Framework

This directory contains tests for the ML Systems Evaluation Framework, designed to ensure reliability and safety for Industrial AI systems.

## Test Structure

### üß™ Unit Tests
- **Core Framework**: Tests for core evaluation logic (`test_core.py`)
- **Collectors**: Tests for data collection components (`test_collectors.py`)
- **Evaluators**: Tests for evaluation engines (`test_evaluators.py`)
- **Reports**: Tests for report generation (`test_reports.py`)
- **CLI**: Tests for command-line interface (`test_cli.py`)

### üîÑ Integration Tests
- **End-to-End**: Complete evaluation pipeline tests (`test_integration.py`)
- **Industry Templates**: Tests for industry-specific configurations
- **CLI Commands**: Tests for command-line interface
- **Error Handling**: Tests for error scenarios and recovery

### üè≠ Industry-Specific Tests
- **Manufacturing**: Quality control and predictive maintenance scenarios
- **Aviation**: Safety-critical system validation
- **Energy**: Grid optimization and demand prediction

### üîí Safety and Compliance Tests
- **Safety Validation**: Tests for safety-critical requirements
- **Compliance Verification**: Tests for regulatory compliance
- **Error Budget Management**: Tests for error budget calculations
- **Incident Response**: Tests for incident handling procedures

## Running Tests

### Prerequisites
```bash
# Install dependencies
uv sync --group dev

# (Optional) Activate the UV-managed virtual environment
uv shell
```

### Basic Test Execution
```bash
# Run all tests
uv run pytest

# Run with verbose output
uv run pytest -v

# Run with coverage report
uv run pytest --cov=ml_eval --cov-report=html --cov-report=term
```

### Using the Test Runner
```bash
# Run all tests
python tests/run_tests.py

# Run specific test types
python tests/run_tests.py --type unit
python tests/run_tests.py --type integration
python tests/run_tests.py --type core
python tests/run_tests.py --type collectors
python tests/run_tests.py --type evaluators
python tests/run_tests.py --type cli
python tests/run_tests.py --type reports

# Run with verbose output
python tests/run_tests.py --verbose

# Run with coverage
python tests/run_tests.py --coverage
```

### Running Specific Test Files
```bash
# Run core tests only
uv run pytest tests/test_core.py

# Run collectors tests only
uv run pytest tests/test_collectors.py

# Run integration tests only
uv run pytest tests/test_integration.py
```

### Running Specific Test Classes
```bash
# Run specific test class
uv run pytest tests/test_core.py::TestEvaluationFramework

# Run specific test method
uv run pytest tests/test_core.py::TestEvaluationFramework::test_framework_creation
```

## Test Categories

### Core Framework Tests (`test_core.py`)
- **CriticalityLevel**: Criticality level validation
- **ComplianceStandard**: Compliance standard validation
- **SLOConfig**: Service Level Objective configuration
- **ErrorBudget**: Error budget tracking and alerts
- **MetricData**: Metric data container
- **EvaluationResult**: Evaluation result aggregation
- **EvaluationFramework**: Main framework orchestration

### Collector Tests (`test_collectors.py`)
- **BaseCollector**: Abstract collector interface
- **OnlineCollector**: Real-time metric collection
- **OfflineCollector**: Historical data collection
- **EnvironmentalCollector**: Environmental condition monitoring
- **RegulatoryCollector**: Compliance data collection
- **Integration**: Multiple collector coordination

### Evaluator Tests (`test_evaluators.py`)
- **BaseEvaluator**: Abstract evaluator interface
- **ReliabilityEvaluator**: System reliability assessment
- **SafetyEvaluator**: Safety-critical system evaluation
- **PerformanceEvaluator**: Performance metrics evaluation
- **ComplianceEvaluator**: Regulatory compliance assessment
- **DriftEvaluator**: Data and concept drift detection
- **Integration**: Multiple evaluator coordination

### CLI Tests (`test_cli.py`)
- **Parser**: Command-line argument parsing
- **Commands**: Individual command execution
- **Error Handling**: Error scenarios and recovery
- **Integration**: End-to-end CLI workflows

### Report Tests (`test_reports.py`)
- **BaseReport**: Abstract report interface
- **ReliabilityReport**: Reliability assessment reports
- **SafetyReport**: Safety violation reports
- **ComplianceReport**: Regulatory compliance reports
- **BusinessImpactReport**: Business impact assessment
- **Integration**: Multiple report type coordination

### Integration Tests (`test_integration.py`)
- **End-to-End**: Complete evaluation pipeline
- **Industry Scenarios**: Industry-specific workflows
- **Error Handling**: System failure recovery
- **Configuration**: Configuration management
- **Performance**: Scalability and performance

## Test Fixtures

The test suite includes fixtures in `conftest.py`:

### Configuration Fixtures
- `sample_config`: Basic system configuration
- `safety_critical_config`: Safety-critical system configuration
- `manufacturing_config`: Manufacturing industry configuration
- `aviation_config`: Aviation industry configuration

### Data Fixtures
- `sample_metrics`: Sample metric data
- `sample_slos`: Sample SLO configurations
- `safety_slos`: Safety-critical SLO configurations
- `error_budget`: Sample error budget
- `evaluation_result`: Sample evaluation result

### Mock Fixtures
- `mock_collector`: Mock collector for testing
- `mock_evaluator`: Mock evaluator for testing
- `temp_config_file`: Temporary configuration file creator

### Environmental Fixtures
- `sample_environmental_conditions`: Environmental sensor data
- `sample_compliance_data`: Compliance standard data

## Test Coverage

The test suite provides coverage for:

### Core Components
- Type definitions and enums
- Configuration classes
- Framework orchestration
- Error handling and validation

### Data Collection
- Online metric collection
- Offline data processing
- Environmental monitoring
- Regulatory compliance data

### Evaluation Engines
- Reliability assessment
- Safety validation
- Performance evaluation
- Compliance verification
- Drift detection

### Reporting
- Multiple report formats (JSON, HTML)
- Industry-specific reports
- Compliance documentation

### CLI Interface
- Command parsing
- Argument validation
- Error handling
- Integration workflows

## Industry-Specific Testing

### Manufacturing
- Quality control defect detection
- Predictive maintenance scenarios
- Real-time monitoring workflows

### Aviation
- Safety-critical decision validation
- Flight control system evaluation
- Environmental condition monitoring
- Regulatory compliance (DO-178C)

### Energy
- Grid optimization scenarios
- Demand prediction workflows
- Real-time monitoring

## Continuous Integration

The test suite is designed for continuous integration with:

### Automated Testing
- Unit test execution
- Integration test validation
- Coverage reporting

### Quality Gates
- Minimum coverage thresholds
- Safety validation requirements
- Compliance verification

### Reporting
- Test result aggregation
- Coverage reports
- Performance metrics

## Best Practices

### Test Organization
- Group related tests in classes
- Use descriptive test names
- Include docstrings for complex tests
- Follow AAA pattern (Arrange, Act, Assert)

### Mock Usage
- Mock external dependencies
- Use realistic test data
- Verify mock interactions
- Clean up after tests

### Error Testing
- Test error conditions
- Verify error messages
- Test recovery scenarios
- Validate error handling

### Performance Testing
- Test with large datasets
- Measure execution time
- Test memory usage
- Validate scalability

## Troubleshooting

### Common Issues
1. **Import Errors**: Ensure the framework is installed in development mode
2. **Mock Failures**: Check mock setup and verification
3. **Configuration Errors**: Validate test configuration files
4. **Performance Issues**: Check test data size and complexity

### Debugging
```bash
# Run tests with debug output
uv run pytest -v -s

# Run specific failing test
uv run pytest tests/test_core.py::TestEvaluationFramework::test_framework_creation -v -s

# Run with coverage and debug
uv run pytest --cov=ml_eval --cov-report=term-missing -v
```

### Test Data
- Test data is generated dynamically
- Use realistic but safe test values
- Avoid hardcoded test data
- Use fixtures for reusable data

## Contributing

When adding new tests:

1. **Follow existing patterns**: Use similar structure and naming
2. **Add comprehensive coverage**: Test all code paths
3. **Include edge cases**: Test error conditions and boundaries
4. **Update documentation**: Keep this README current
5. **Run full test suite**: Ensure no regressions

### Test Naming Convention
- Test classes: `Test<ComponentName>`
- Test methods: `test_<description>`
- Integration tests: `test_<scenario>_<outcome>`

### Test Data Convention
- Use fixtures for reusable data
- Generate realistic test data
- Avoid hardcoded values
- Include edge cases and error conditions
